{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e5fcb1-525e-4ffc-beab-8810654a297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feb5985",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db7e242-6bff-4059-bcd1-9eee60fb1f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a88320-a825-46db-8f2c-5147559a2ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ICV_to_grayscale(image):\n",
    "    w, h, c = image.shape\n",
    "    grayimg = np.zeros((w,h))\n",
    "    grayimg = (0.299*image[:,:,0] + 0.587*image[:,:,1] + 0.114*image[:,:,2])\n",
    "    return grayimg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af7a3c7-4f7b-4f5c-a917-c64c43bd24fa",
   "metadata": {},
   "source": [
    "# Task A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63dee30-1ea2-4da1-b862-182238fa0c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'DatasetC.mpg'\n",
    "cap = cv2.VideoCapture(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741b4de1",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "1. Classification Threshold: I picked a classification threshold of 40 as anything lower mean the frame difference images were too noisy and any higher of a threshold and the image didn't have enough information in the frame differences.\n",
    "2. Frame difference: Take pixel-by-pixel difference between 2 images (ideally frames from a video). We take the absolute value of the difference as images can't have negative pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeb75b7-ac4e-46b7-9a53-b2b8a9f23275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ICV_classification_threshold(fdif):\n",
    "    threshold = 40\n",
    "    return np.where(fdif > threshold, 255, 0)\n",
    "        \n",
    "def ICV_frame_difference(frame1, frame):\n",
    "    return np.abs(frame1 - frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5614081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ICV_get_video_frames(video_capture):\n",
    "    if not video_capture.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        exit()\n",
    "    \n",
    "    frames = []\n",
    "    \n",
    "    frame_count = 0\n",
    "    total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(\"Total Frames: \", total_frames)\n",
    "    while frame_count < total_frames:\n",
    "        ret, frame = video_capture.read()\n",
    "        if ret:\n",
    "            frames.append(frame)\n",
    "        frame_count += 1\n",
    "    \n",
    "    video_capture.release()\n",
    "    return frames\n",
    "\n",
    "video_path = 'DatasetC.avi'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frames = ICV_get_video_frames(cap)\n",
    "print(len(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9884dc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(10, 10))\n",
    "selected_frame_1, selected_frame_2 = 34, 90 \n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.axis('off')\n",
    "\n",
    "axs[0].imshow(frames[0])\n",
    "axs[0].set_title(\"Reference Frame\")\n",
    "axs[1].imshow(frames[selected_frame_1])\n",
    "axs[1].set_title(\"Selected Frame 1\")\n",
    "axs[2].imshow(frames[selected_frame_2])\n",
    "axs[2].set_title(\"Selected Frame 2\")\n",
    "plt.savefig(\"SelectedFrames5a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d814c90-b9db-4778-9c15-b0f24bb0b0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ICV_frame_difference_with_first_frame(video_capture):\n",
    "    \"\"\"\n",
    "    This function get a pixel-by-pixel difference of each frame within the video-capture. \n",
    "    Returns the list of the frame difference and the differences with a classification threshold applied to them.\n",
    "\n",
    "    Parameters:\n",
    "    video_capture -> cv2.VideoCapture(\"File\")\n",
    "\n",
    "    Returns:\n",
    "    fdiffs, fdiffs_with_threshold\n",
    "    \"\"\"\n",
    "\n",
    "    if not video_capture.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        exit()\n",
    "    \n",
    "    frame_count = 1\n",
    "    f_diffs, fdiffs_with_threshold = [], []\n",
    "    # Get Max number of frames\n",
    "    total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(\"Total Frames: \", total_frames)\n",
    "    # Get first frame as reference\n",
    "    ret, frame1 = video_capture.read()\n",
    "    print(frame1.shape)\n",
    "    # Convert frame to grayscale\n",
    "    fg1 = ICV_to_grayscale(frame1)\n",
    "\n",
    "    # Iterate through all remaining frames in the video capture\n",
    "    while frame_count < total_frames:\n",
    "        ret, frame = video_capture.read() # Get current frame\n",
    "        if ret:\n",
    "            fg = ICV_to_grayscale(frame) # convert it to grascale\n",
    "            fdif = ICV_frame_difference(fg1, fg) # get the frame difference\n",
    "            f_diffs.append(fdif) \n",
    "            fdif1 = ICV_classification_threshold(fdif) # apply classification threshold to the frame difference\n",
    "            fdiffs_with_threshold.append(fdif1)\n",
    "        frame_count += 1\n",
    "    \n",
    "    video_capture.release() # Release video capture\n",
    "    return f_diffs, fdiffs_with_threshold\n",
    "\n",
    "\n",
    "video_path = 'DatasetC.avi'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_difs, frame_difs_with_threshold = np.array(ICV_frame_difference_with_first_frame(cap))\n",
    "print(\"Done\")\n",
    "frame_difs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4358c2f-ca58-45fa-86c2-46ccf5ee7bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.axis('off')\n",
    "\n",
    "axs[0][0].imshow(frame_difs[selected_frame_1 - 1])\n",
    "axs[0][0].set_title(\"Frame Difference for Selected Frame 1\")\n",
    "axs[0][1].imshow(frame_difs[selected_frame_2 - 1])\n",
    "axs[0][1].set_title(\"Frame Difference for Selected Frame 2\")\n",
    "axs[1][0].imshow(frame_difs_with_threshold[selected_frame_1 - 1])\n",
    "axs[1][0].set_title(\"Frame Difference with Threshold for Selected Frame 1\")\n",
    "axs[1][1].imshow(frame_difs_with_threshold[selected_frame_2 - 1])\n",
    "axs[1][1].set_title(\"Frame Difference with Threshold for Selected Frame 2\")\n",
    "fig.suptitle(\"Frame Differences\", fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"FrameDifferences5a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a26206e-21f7-4692-8dc2-4f0fde1d5edd",
   "metadata": {},
   "source": [
    "# Task B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f97bec-fb8e-4f2c-ad38-485b45901e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ICV_frame_difference_with_previous_frame(video_capture):\n",
    "    \"\"\"\n",
    "    This function get a pixel-by-pixel difference of consecutive frame within the video-capture. \n",
    "    Returns the list of the frame difference and the differences with a classification threshold applied to them.\n",
    "\n",
    "    Parameters:\n",
    "    video_capture -> cv2.VideoCapture(\"File\")\n",
    "\n",
    "    Returns:\n",
    "    fdiffs, fdiffs_with_threshold -> List[List[List[[int]]]], List[List[List[[int]]]]\n",
    "    \"\"\"\n",
    "\n",
    "    if not video_capture.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        exit()\n",
    "    \n",
    "    frame_count = 0\n",
    "    frames, f_diffs, fdiffs_with_threshold = [], [], []\n",
    "    # Get max number of frames\n",
    "    total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(\"Total Frames: \", total_frames)\n",
    "    # Iterate through all frames from the beginning and add them to a list of frames and convert each frame to grayscale\n",
    "    while frame_count < total_frames:\n",
    "        ret, frame = video_capture.read()\n",
    "        if ret:\n",
    "            fg = ICV_to_grayscale(frame)\n",
    "            frames.append(frame)\n",
    "        frame_count += 1\n",
    "\n",
    "    # Iterate through list of all frames and grab consecutive frames\n",
    "    for i in range(len(frames)-1):\n",
    "        fi, fi1 = frames[i], frames[i+1]\n",
    "        fdif = ICV_frame_difference(fi, fi1) # Get difference of consecutive frames\n",
    "        f_diffs.append(fdif)\n",
    "        fdif1 = ICV_classification_threshold(fdif) # apply classification threshold\n",
    "        fdiffs_with_threshold.append(fdif1)\n",
    "    \n",
    "    video_capture.release() # Release Video Capture\n",
    "    return f_diffs, fdiffs_with_threshold\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_difs_consecutive, fdiffs_with_threshold_consecutive = np.array(ICV_frame_difference_with_previous_frame(cap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162516b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 10))\n",
    "selected_frame_1, selected_frame_2 = 50, 111 \n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.axis('off')\n",
    "\n",
    "axs[0].imshow(frames[selected_frame_1])\n",
    "axs[0].set_title(\"Selected Frame 1\")\n",
    "axs[1].imshow(frames[selected_frame_2])\n",
    "axs[1].set_title(\"Selected Frame 2\")\n",
    "\n",
    "plt.savefig(\"SelectedFrame5b.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fa13bf-bf0e-45bf-9421-ca0a7482fcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.axis('off')\n",
    "\n",
    "axs[0][0].imshow(frame_difs_consecutive[selected_frame_1 - 1])\n",
    "axs[0][0].set_title(\"Frame Difference for Selected Frame 1\")\n",
    "axs[0][1].imshow(frame_difs_consecutive[selected_frame_2 - 1])\n",
    "axs[0][1].set_title(\"Frame Difference for Selected Frame 2\")\n",
    "axs[1][0].imshow(fdiffs_with_threshold_consecutive[selected_frame_1 - 1])\n",
    "axs[1][0].set_title(\"Frame Difference with Threshold for Selected Frame 1\")\n",
    "axs[1][1].imshow(fdiffs_with_threshold_consecutive[selected_frame_2 - 1])\n",
    "axs[1][1].set_title(\"Frame Difference with Threshold for Selected Frame 2\")\n",
    "fig.suptitle(\"Frame Differences\", fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"FrameDifferences5b.png\")\n",
    "\n",
    "# The result frames here look distorted which was an issue which cropped up after refactoring all functions to start with ICV_\n",
    "# The report has the correct version for consecutive frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc94856-be66-481e-9513-83afe45ff010",
   "metadata": {},
   "source": [
    "# Task C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf61d3eb-b867-4433-807d-8f9e2d3f9c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "def ICV_find_background(video_capture):\n",
    "    \"\"\"\n",
    "    Applies Weighted temporal averaging on all frames of a video sequence to generate a refence frame for all objects which show in all frames of the video.\n",
    "    This way will not have to use the first frame of the video anymore. \n",
    "\n",
    "    Parameters:\n",
    "    video_capture -> cv2.VideoCapture(\"File\")\n",
    "\n",
    "    Returns:\n",
    "    background -> List[List[[int]]]\n",
    "    \"\"\"\n",
    "    def ICV_weighted_temporal_average(background, c_frame):\n",
    "        alpha = 0.1\n",
    "        return (1-alpha)*background + alpha*c_frame\n",
    "    \n",
    "    if not video_capture.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        exit()\n",
    "    \n",
    "    frame_count = 1\n",
    "    ret, frame = video_capture.read()\n",
    "    background = ICV_to_grayscale(frame)\n",
    "    total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(\"Total Frames: \", total_frames)\n",
    "    while frame_count < total_frames:\n",
    "        ret, c_frame = video_capture.read()\n",
    "        if ret:\n",
    "            c_frame = ICV_to_grayscale(c_frame)\n",
    "            background = ICV_weighted_temporal_average(background, c_frame)\n",
    "        frame_count += 1\n",
    "    \n",
    "    video_capture.release()\n",
    "    return background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ecebf3-c433-47e2-bd38-a2f3d2c91fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg = ICV_find_background(cap)\n",
    "plt.imshow(bg, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d868d593-ed9e-44f6-b734-5115f8701375",
   "metadata": {},
   "source": [
    "# Task D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bce401-2d29-4948-884a-b5bc9abe5b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ICV_frame_difference_with_background(video_capture, background):\n",
    "    \"\"\"\n",
    "    This function get a pixel-by-pixel difference of each frame in the video against the generated background we created.\n",
    "\n",
    "    Parameters:\n",
    "    video_capture -> cv2.VideoCapture(\"File\")\n",
    "\n",
    "    Returns:\n",
    "    fdiffs -> List[List[List[[int]]]]\n",
    "    \"\"\"\n",
    "\n",
    "    if not video_capture.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        exit()\n",
    "    \n",
    "    frame_count = 0\n",
    "    f_difs_bg = []\n",
    "    # Max Frame count\n",
    "    total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(\"Total Frames: \", total_frames)\n",
    "    # Iterate over all frames\n",
    "    while frame_count < total_frames:\n",
    "        ret, c_frame = video_capture.read() # Get current frame\n",
    "        if ret:\n",
    "            c_frame = ICV_to_grayscale(c_frame) # convert it to grayscale\n",
    "            dif = ICV_frame_difference(background, c_frame) # Take the difference with the background\n",
    "            dif = ICV_classification_threshold(dif) # apply classification threshold\n",
    "            f_difs_bg.append(dif)\n",
    "        frame_count += 1\n",
    "    video_capture.release()\n",
    "    return f_difs_bg\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "f_difs_bg = ICV_frame_difference_with_background(cap, bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcb06dd-7a87-4a25-b689-5c096043e311",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(f_difs_bg), 20):\n",
    "    plt.imshow(f_difs_bg[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3b5ef3-5c63-4489-978e-970e66f2f986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ICV_flood_fill(image, x, y, visited):\n",
    "    \"\"\"\n",
    "    This algorithm takes a binary image and checks for pixels with non visited and non zero values. \n",
    "    It keeps searching for neighboring values from some starting point (x, y), until there are no more values left to visit in its stack.\n",
    "\n",
    "    Parameters:\n",
    "    image -> List[List[int]] (Serves as matrix or graph within this algoritm is searching for similar neighboring pixels)\n",
    "    x -> int (starting x coordinate)\n",
    "    y -> int (starting y coordinate)\n",
    "    visited -> List[int] (helps keep track of all positions which have been visited when searching for neighboring points from some point (x, y))\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    width, height = image.shape\n",
    "    stack = [(x,y)]\n",
    "    while stack:\n",
    "        x, y = stack.pop()\n",
    "        if x >= 0 and y >= 0 and x < width and y < height:\n",
    "            if visited[x, y] or image[x, y] == 0:\n",
    "                continue \n",
    "            else:\n",
    "                visited[x, y] = True\n",
    "                stack.extend([(x-1, y-1), (x, y-1), (x+1, y-1), (x-1, y), (x+1, y), (x+1, y+1), (x, y+1), (x+1, y+1)])\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "\n",
    "def ICV_count_moving_objects(image):\n",
    "    \"\"\"\n",
    "    Takes an image and iterates over each pixel and applies flood fill algorithm for each position. \n",
    "    Incrememnts a count for number of objects, each time the flood fill algorithm completes execution.\n",
    "\n",
    "    Parameters: \n",
    "    image -> Listp[List[int]]\n",
    "\n",
    "    Returns:\n",
    "    object_count -> int\n",
    "    \"\"\"\n",
    "    width, height = image.shape\n",
    "    visited = np.full((image.shape[0], image.shape[1]), False)\n",
    "    object_count = 0\n",
    "    for i in range(width):\n",
    "        for j in range(height):\n",
    "            if image[i,j]==255 and not visited[i,j]:\n",
    "                ICV_flood_fill(image, i, j, visited)\n",
    "                object_count += 1\n",
    "    return object_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2262ae-b226-4fb8-9ba9-2113da357bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_objects_in_video = []\n",
    "for i in range(0, len(f_difs_bg), 1):\n",
    "    moc = ICV_count_moving_objects(f_difs_bg[i])\n",
    "    moving_objects_in_video.append(moc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3139fd45-23ab-4478-9268-63d681bce817",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(0,140,1), moving_objects_in_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b1fb4f-14a2-43e6-abfe-dbf7be91c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Taken from Question 2 to apply denoising on the video frames\n",
    "\n",
    "def ICV_convolution_filtering_grayscale(image, kernel):\n",
    "    \"\"\"\n",
    "    Applies a convolution kernel on a greyscale image.\n",
    "\n",
    "    Parameters:\n",
    "    image -> 2D numpy array\n",
    "    kernel -> 2D numpy array (usually with shapes (3,3), (5,5) or (7,7))\n",
    "\n",
    "    Returns:\n",
    "    2D numpy array (Image with filter applied to it)\n",
    "    \"\"\"\n",
    "    width, height = image.shape[:2] # Get width and height from image shape\n",
    "    new_image = np.zeros((width, height)) # create new image array\n",
    "    k_w, k_h = kernel.shape[:2] # Get width and height of convolution kernel\n",
    "\n",
    "    ## I pad the image with zeroes to half the size of the \n",
    "    ## kernel's width/height before applying the filter\n",
    "    pad_width, pad_height = (k_w-1)//2, (k_h-1)//2 \n",
    "\n",
    "    ## The padded image is slightly larger to account for the pad width/height\n",
    "    ## As the image is padded at the top and bottom and left and right\n",
    "    padded_image = np.zeros((width + 2 * pad_width, height + 2 * pad_height))\n",
    "\n",
    "    ## Put the original image back into the larger padded array\n",
    "    padded_image[pad_width:pad_width + width, pad_height:pad_height + height] = image\n",
    "\n",
    "    ## When iterating over the image to ensure that the kernel is properly applied\n",
    "    ## and so we don't get out of bounds errors, I iterate till width/height -pad_width/pad_height\n",
    "    for i in range(width-pad_width):\n",
    "        for j in range(height-pad_height):\n",
    "            ## Apply the kernel on each pixel of the original image \n",
    "            ## and store the results in the new_image array\n",
    "            new_image[i+1, j+1] = np.sum(kernel * padded_image[i:i+k_w, j:j+k_h])\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a9a81f-1bff-4aab-a5bc-734b9cb6da38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ICV_gaussian_blur(image):\n",
    "    # Applies Gaussian Blur convolution kernel to an image and returns the filtered image\n",
    "    kernel = np.array([[1,2,1],[2,4,2], [1,2,1]])\n",
    "    return ICV_convolution_filtering_grayscale(image, kernel)/np.sum(kernel)\n",
    "\n",
    "def ICV_mean_filter(image):\n",
    "    # Applies Mean convolution kernel to an image and returns the filtered image\n",
    "    kernel = np.ones((3,3))\n",
    "    return ICV_convolution_filtering_grayscale(image, kernel)/np.sum(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4ba58f-040b-4ca0-a6db-51d48121fe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "denoised_img1 = ICV_mean_filter(f_difs_bg[0])\n",
    "denoised_img2 = ICV_gaussian_blur(f_difs_bg[0])\n",
    "plt.imshow(denoised_img1)\n",
    "plt.show()\n",
    "plt.imshow(denoised_img2)\n",
    "plt.show()\n",
    "\n",
    "moc = ICV_count_moving_objects(f_difs_bg[0])\n",
    "moc1 = ICV_count_moving_objects(denoised_img1)\n",
    "moc2 = ICV_count_moving_objects(denoised_img2)\n",
    "moc, moc1, moc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b61a2c9-b01e-4938-afca-9b288bc6d12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_objects_in_video = []\n",
    "for i in range(0, len(f_difs_bg), 1):\n",
    "    moc = ICV_count_moving_objects(f_difs_bg[i])\n",
    "    moving_objects_in_video.append(moc)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c43c59d-3f60-4846-ac91-7e979a07c269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can take up to 3-4 minutes to compute\n",
    "moving_objects_in_video1 = []\n",
    "for i in range(0, len(f_difs_bg), 1):\n",
    "    moc1 = ICV_count_moving_objects(ICV_mean_filter(f_difs_bg[i])) # Apply mean filter for denoising the frame\n",
    "    moving_objects_in_video1.append(moc1)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e89d6da-97c3-47e8-affc-ae5b7da1d440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can take up to 3-4 minutes to compute\n",
    "moving_objects_in_video2 = []\n",
    "for i in range(0, len(f_difs_bg), 1):\n",
    "    moc2 = ICV_count_moving_objects(ICV_gaussian_blur(f_difs_bg[i])) # Apply gaussian blur for denoising the frame\n",
    "    moving_objects_in_video2.append(moc2)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af1c7fd-cccd-4d50-a2c9-bdd2a843cb07",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.bar(range(0,140,1), moving_objects_in_video) # Plot raw object counts for each frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6569be-059a-4390-a72c-a979f6fc08c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(0,140,1), moving_objects_in_video1) # Plot mean filtered object counts for each frame (denoizing of each frame using mean filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52ff3a0-f432-4291-886f-983dd9293a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(0,140,1), moving_objects_in_video2) # Plot gaussian blur filtered object counts for each frame (denoizing of each frame using gaussian blur filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a11a06f-3ca8-41f0-87cf-bc16f086bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(moving_objects_in_video), len(moving_objects_in_video1), len(moving_objects_in_video2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
